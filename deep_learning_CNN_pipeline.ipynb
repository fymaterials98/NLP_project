{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is CNN model for NLP, tell the difference between data scientist and other engineer title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/data_data_scientist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./data/data_engineer.csv', nrows =25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.concat([df1, df2], sort =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop =True, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_scientist(title_value):\n",
    "    target = 'data scientist'\n",
    "    if type(title_value) == float:\n",
    "        return -1\n",
    "    elif type(title_value) ==str:\n",
    "        if title_value.lower().find(target) >=0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data_scientist =df.job_title.map(find_data_scientist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, -1,  0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_data_scientist.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_scientist_binary(title_value):\n",
    "    target = 'data scientist'\n",
    "    if type(title_value) == float:\n",
    "        return 0\n",
    "    elif type(title_value) ==str:\n",
    "        if title_value.lower().find(target) >=0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data_scientist =df.job_title.map(find_data_scientist_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add is_data_scientist column to indicate whether the title has data scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_data_scientist']=is_data_scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.dropna(subset =['job_description'])\n",
    "df_new.reset_index(drop =True, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>additional_information</th>\n",
       "      <th>job_ID</th>\n",
       "      <th>job_company</th>\n",
       "      <th>job_date</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_title</th>\n",
       "      <th>person_ID</th>\n",
       "      <th>skills</th>\n",
       "      <th>is_data_scientist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Core CompetenciesData Modeling &amp; Visualization...</td>\n",
       "      <td>1</td>\n",
       "      <td>TRILLO INC - San Francisco Bay Area, CA</td>\n",
       "      <td>January 2018 to Present</td>\n",
       "      <td>* Incorporate Machine Learning solutions to fa...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>1</td>\n",
       "      <td>CLUSTERING (Less than 1 year), DATA MINING (Le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TECHNICAL EXPERTISEStatistics/Machine Learning...</td>\n",
       "      <td>1</td>\n",
       "      <td>Pepsico - Plano, TX</td>\n",
       "      <td>September 2018 to Present</td>\n",
       "      <td>Fleet Analysis: Exploring the Truck repair and...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Hadoop (Less than 1 year), PYTHON (Less than 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TECHNICAL EXPERTISEStatistics/Machine Learning...</td>\n",
       "      <td>2</td>\n",
       "      <td>CISCO - San Jose, CA</td>\n",
       "      <td>May 2018 to July 2018</td>\n",
       "      <td>Developed Python scripts to automate the load...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Hadoop (Less than 1 year), PYTHON (Less than 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TECHNICAL EXPERTISEStatistics/Machine Learning...</td>\n",
       "      <td>3</td>\n",
       "      <td>PHOTON INFOTECH - Bohemia, NY</td>\n",
       "      <td>September 2017 to March 2018</td>\n",
       "      <td>Developed a personalized recommender engine fo...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Hadoop (Less than 1 year), PYTHON (Less than 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TECHNICAL EXPERTISEStatistics/Machine Learning...</td>\n",
       "      <td>4</td>\n",
       "      <td>APPLE INC - Cupertino, CA</td>\n",
       "      <td>January 2016 to August 2017</td>\n",
       "      <td>Played a key role in developing and maintainin...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Hadoop (Less than 1 year), PYTHON (Less than 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              additional_information  job_ID  \\\n",
       "0  Core CompetenciesData Modeling & Visualization...       1   \n",
       "1  TECHNICAL EXPERTISEStatistics/Machine Learning...       1   \n",
       "2  TECHNICAL EXPERTISEStatistics/Machine Learning...       2   \n",
       "3  TECHNICAL EXPERTISEStatistics/Machine Learning...       3   \n",
       "4  TECHNICAL EXPERTISEStatistics/Machine Learning...       4   \n",
       "\n",
       "                               job_company                      job_date  \\\n",
       "0  TRILLO INC - San Francisco Bay Area, CA       January 2018 to Present   \n",
       "1                      Pepsico - Plano, TX     September 2018 to Present   \n",
       "2                     CISCO - San Jose, CA         May 2018 to July 2018   \n",
       "3            PHOTON INFOTECH - Bohemia, NY  September 2017 to March 2018   \n",
       "4                APPLE INC - Cupertino, CA   January 2016 to August 2017   \n",
       "\n",
       "                                     job_description       job_title  \\\n",
       "0  * Incorporate Machine Learning solutions to fa...  Data Scientist   \n",
       "1  Fleet Analysis: Exploring the Truck repair and...  Data Scientist   \n",
       "2   Developed Python scripts to automate the load...  Data Scientist   \n",
       "3  Developed a personalized recommender engine fo...  Data Scientist   \n",
       "4  Played a key role in developing and maintainin...  Data Scientist   \n",
       "\n",
       "   person_ID                                             skills  \\\n",
       "0          1  CLUSTERING (Less than 1 year), DATA MINING (Le...   \n",
       "1          2  Hadoop (Less than 1 year), PYTHON (Less than 1...   \n",
       "2          2  Hadoop (Less than 1 year), PYTHON (Less than 1...   \n",
       "3          2  Hadoop (Less than 1 year), PYTHON (Less than 1...   \n",
       "4          2  Hadoop (Less than 1 year), PYTHON (Less than 1...   \n",
       "\n",
       "   is_data_scientist  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46773, 9)\n"
     ]
    }
   ],
   "source": [
    "print(df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6321"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_new.is_data_scientist ==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "#from keras.layers import LSTM, Bidirectional\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import gensim\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below code is a test for concat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = Input(shape=(10,20))\n",
    "inp2 = Input(shape=(10,32))\n",
    "cc1 = concatenate([inp1, inp2], axis=2) # Merge column, same row\n",
    "output = Dense(30, activation='relu')(cc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[inp1, inp2], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[0].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = [[1, 2, 3], [4, 5, 6]]\n",
    "t2 = [[7, 8, 9], [10, 11, 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = Input(shape=(2,3))\n",
    "inp2 = Input(shape=(2,3))\n",
    "cc1 = concatenate([inp1, inp2], axis=2) # Merge column, same row\n",
    "output = Dense(30, activation='relu')(cc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[inp1, inp2], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above is for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./data\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 150000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 200 # max number of words in a comment to use\n",
    "\n",
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_new['job_description'].map(lambda x: x.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##define a class, which has fit, transform and fit_transform function\n",
    "class Convert_pad_sequence(object):\n",
    "    def __init__(self):\n",
    "        self.EMBEDDING_DIM = 300 # how big is each word vector\n",
    "        self.MAX_VOCAB_SIZE = 100000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "        self.MAX_SEQUENCE_LENGTH = 200 # max number of words in a comment to use\n",
    "\n",
    "        #training params\n",
    "        #self.batch_size = 256 \n",
    "        #self.num_epochs = 2 \n",
    "        \n",
    "        #load word2vect dictionary\n",
    "        word2vec_path = \"./data/GoogleNews-vectors-negative300.bin.gz\"\n",
    "        self.word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, \\\n",
    "                                                                        binary=True, limit=300000)\n",
    "    \n",
    "    def fit(self, input_data_series, y =None):\n",
    "        if y.size:\n",
    "            array_size = y.shape\n",
    "            if len(array_size)==1:\n",
    "                self.labels_index =1\n",
    "            else:\n",
    "                self.labels_index = array_size[1]\n",
    "                \n",
    "        self.tokenizer =Tokenizer(num_words=self.MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "        self.tokenizer.fit_on_texts(input_data_series.tolist())\n",
    "        train_word_index = self.tokenizer.word_index\n",
    "        self.len_index = len(train_word_index)\n",
    "        self.train_embedding_weights = np.zeros((self.len_index+1, self.EMBEDDING_DIM))\n",
    "        \n",
    "        #training_sequences = self.tokenizer.texts_to_sequences(input_data_frame[\"job_description\"].tolist())\n",
    "        self.train_embedding_weights = np.zeros((self.len_index+1, self.EMBEDDING_DIM))\n",
    "        #train_embedding_weights = np.zeros((MAX_VOCAB_SIZE+1, EMBEDDING_DIM))\n",
    "        for word,index in train_word_index.items():\n",
    "            self.train_embedding_weights[index,:] = self.word2vec[word] if \\\n",
    "                                               word in self.word2vec else np.random.rand(self.EMBEDDING_DIM)\n",
    "            \n",
    "    def transform(self, input_data_series, y =None):\n",
    "        training_sequences = self.tokenizer.texts_to_sequences(input_data_series.tolist())\n",
    "        train_cnn_data = pad_sequences(training_sequences, maxlen=self.MAX_SEQUENCE_LENGTH)\n",
    "        \n",
    "        #return a tuple\n",
    "        return (train_cnn_data, self.train_embedding_weights, \n",
    "                self.MAX_SEQUENCE_LENGTH, self.len_index+1,\n",
    "                self.EMBEDDING_DIM, self.labels_index)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def fit_transform(self, input_data_series, y=None):\n",
    "        self.fit(input_data_series, y)\n",
    "        \n",
    "        return self.transform(input_data_series)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_text(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _ConvNet(self,embeddings, max_sequence_length, num_words, embedding_dim, \\\n",
    "                labels_index, trainable=False, extra_conv=True):\n",
    "    \n",
    "        embedding_layer = Embedding(num_words,\n",
    "                                   embedding_dim,\n",
    "                                   weights=[embeddings],\n",
    "                                   input_length=max_sequence_length,\n",
    "                                   trainable=trainable)\n",
    "\n",
    "        sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "        # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "        convs = []\n",
    "        filter_sizes = [3,4,5]\n",
    "\n",
    "        for filter_size in filter_sizes:\n",
    "            l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "            l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "            convs.append(l_pool)\n",
    "\n",
    "        #l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "        l_merge = concatenate(convs, axis=1)\n",
    "\n",
    "        # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "        conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "        pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "        if extra_conv==True:\n",
    "            x = Dropout(0.5)(l_merge)  \n",
    "        else:\n",
    "            # Original Yoon Kim model\n",
    "            x = Dropout(0.5)(pool)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        # Finally, we feed the output into a Sigmoid layer.\n",
    "        # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
    "        # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "        preds = Dense(labels_index, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(sequence_input, preds)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def fit(self, input_data, y=None):\n",
    "        x_train = input_data[0]\n",
    "        y_train = y.values\n",
    "        X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "        RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "        embeddings = input_data[1]\n",
    "        max_sequence_length = input_data[2]\n",
    "        num_words = input_data[3]\n",
    "        embedding_dim = input_data[4]\n",
    "        labels_index = input_data[5]\n",
    "        \n",
    "        batch_size = 256\n",
    "        num_epochs = 2\n",
    "        \n",
    "        self.model = self._ConvNet(embeddings, max_sequence_length, num_words, \n",
    "                                   embedding_dim, labels_index, False)\n",
    "        \n",
    "        self.model.fit(X_tra, y_tra, batch_size=batch_size, epochs=num_epochs, \n",
    "                       validation_data=(X_val, y_val),\n",
    "                       callbacks=[RocAuc], verbose=2)\n",
    "        \n",
    "    def transform(self, input_data, y=None):\n",
    "        pass\n",
    "    \n",
    "    def fit_transform(self, input_data, y=None):\n",
    "        self.fit(input_data, y)\n",
    "        self.transform()\n",
    "        \n",
    "    def predict(self, input_data, batch_size=None, verbose=0, steps=None):\n",
    "        return self.model.predict(input_data[0], batch_size, verbose, steps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field, job_label):\n",
    "    text_first =df[text_field].map(lambda x: x.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \"))\n",
    "    text_second = text_first.map(lambda x: x.lower())\n",
    "    #df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    #df[text_field] = df[text_field].str.lower()\n",
    "    df_copy = pd.concat([text_second, df[job_label]], axis =1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments =standardize_text(df_new, 'job_description', 'is_data_scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_description</th>\n",
       "      <th>is_data_scientist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* incorporate machine learning solutions to fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fleet analysis: exploring the truck repair and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>developed python scripts to automate the load...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>developed a personalized recommender engine fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>played a key role in developing and maintainin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     job_description  is_data_scientist\n",
       "0  * incorporate machine learning solutions to fa...                  1\n",
       "1  fleet analysis: exploring the truck repair and...                  1\n",
       "2   developed python scripts to automate the load...                  1\n",
       "3  developed a personalized recommender engine fo...                  1\n",
       "4  played a key role in developing and maintainin...                  1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_VOCAB_SIZE = 94918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_input = C_pad_sequence.transform(test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prob = cnn_text_model.predict(test_input[0])\n",
    "#print('prob is: ',prob[0][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =[('convert_to_pad', Convert_pad_sequence()), \n",
    "        ('cnn_text_model', CNN_text())]\n",
    "model = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fengyi/.pyenv/versions/3.6.4/envs/try_deep_learning/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     31821300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 198, 128)     115328      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 197, 128)     153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 196, 128)     192128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 66, 128)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 65, 128)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 65, 128)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 196, 128)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 196, 128)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 25088)        0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          3211392     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 35,494,005\n",
      "Trainable params: 3,672,705\n",
      "Non-trainable params: 31,821,300\n",
      "__________________________________________________________________________________________________\n",
      "Train on 44434 samples, validate on 2339 samples\n",
      "Epoch 1/2\n",
      " - 371s - loss: 0.5310 - acc: 0.8735 - val_loss: 0.2424 - val_acc: 0.8961\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.922390 \n",
      "\n",
      "Epoch 2/2\n",
      " - 370s - loss: 0.2195 - acc: 0.8959 - val_loss: 0.2192 - val_acc: 0.9059\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.938693 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('convert_to_pad', <__main__.Convert_pad_sequence object at 0x135b40438>), ('cnn_text_model', <__main__.CNN_text object at 0x135b40c50>)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_comments['job_description'], train_comments['is_data_scientist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_string = 'modeling for prediction, use python for programming, linear regression, classification,\\\n",
    "                     collect data, explore, random forest '\n",
    "test_series = pd.Series([test_input_string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob is:  0.53039086\n"
     ]
    }
   ],
   "source": [
    "prob = model.predict(test_series)\n",
    "print('prob is: ', prob[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert_to_pad\n"
     ]
    }
   ],
   "source": [
    "print(model.steps[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Convert_pad_sequence object at 0x135b40438>\n"
     ]
    }
   ],
   "source": [
    "print(model.steps[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.CNN_text object at 0x135b40c50>\n"
     ]
    }
   ],
   "source": [
    "print(model.steps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/cnn_data_scientist_preprocsssing.pkl', 'wb') as f:\n",
    "    pickle.dump(model.steps[0][1],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.steps[1][1].model.save('./data/cnn_data_scientist_modeling.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below different callback mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hist = model.fit(x_train, y_tr, epochs=num_epochs, callbacks=callbacks_list, \n",
    "                 validation_split=0.1, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above different callback mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=num_epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./data/cnn_text_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate plots\n",
    "plt.figure()\n",
    "plt.plot(hist.history['loss'], lw=2.0, color='b', label='train')\n",
    "plt.plot(hist.history['val_loss'], lw=2.0, color='r', label='val')\n",
    "plt.title('CNN sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(hist.history['acc'], lw=2.0, color='b', label='train')\n",
    "plt.plot(hist.history['val_acc'], lw=2.0, color='r', label='val')\n",
    "plt.title('CNN sentiment')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
