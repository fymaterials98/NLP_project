{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process work experience data with NLTK (stemming, lemmatization, stop words) and Sklearn (countvectorizer, tfidfvectorizer, Naive Bayes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/data_scientists.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personal_ID</th>\n",
       "      <th>job_ID</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>working_dates</th>\n",
       "      <th>job_description</th>\n",
       "      <th>skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALGORITHM (少于1年)|BUSINESS REQUIREMENTS (少于1年)|...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Vice President / Fraud Data Scientist</td>\n",
       "      <td>JPMorgan Chase - Wilmington, DE</td>\n",
       "      <td>2015年3月 至 目前</td>\n",
       "      <td>Credit Card POS Transactional Fraud Acquisitio...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sr. Modeler / Data Scientist</td>\n",
       "      <td>American Eagle Outfitters - Pittsburgh, PA</td>\n",
       "      <td>2013年8月 至 2015年3月</td>\n",
       "      <td>Responsibilities Manage all customer modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Quantitative Engagement Manger</td>\n",
       "      <td>Essex Lake Group - New York, NY</td>\n",
       "      <td>2011年12月 至 2013年8月</td>\n",
       "      <td>Responsibilities Present results to executive ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Sr. Risk Analyst</td>\n",
       "      <td>Paypal - 上海市</td>\n",
       "      <td>2011年3月 至 2011年12月</td>\n",
       "      <td>Responsibilities Develop seller risk modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personal_ID  job_ID                              job_title  \\\n",
       "0            1       0                                    NaN   \n",
       "1            2       1  Vice President / Fraud Data Scientist   \n",
       "2            2       2           Sr. Modeler / Data Scientist   \n",
       "3            2       3         Quantitative Engagement Manger   \n",
       "4            2       4                       Sr. Risk Analyst   \n",
       "\n",
       "                                 company_name       working_dates  \\\n",
       "0                                         NaN                 NaN   \n",
       "1             JPMorgan Chase - Wilmington, DE        2015年3月 至 目前   \n",
       "2  American Eagle Outfitters - Pittsburgh, PA   2013年8月 至 2015年3月   \n",
       "3             Essex Lake Group - New York, NY  2011年12月 至 2013年8月   \n",
       "4                                Paypal - 上海市  2011年3月 至 2011年12月   \n",
       "\n",
       "                                     job_description  \\\n",
       "0                                                NaN   \n",
       "1  Credit Card POS Transactional Fraud Acquisitio...   \n",
       "2  Responsibilities Manage all customer modeling ...   \n",
       "3  Responsibilities Present results to executive ...   \n",
       "4  Responsibilities Develop seller risk modeling ...   \n",
       "\n",
       "                                               skill  \n",
       "0  ALGORITHM (少于1年)|BUSINESS REQUIREMENTS (少于1年)|...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.dropna(subset = ['job_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define three functions:\n",
    "1. has_data_scientist, if the input value is a string, has 'data scientist' in it, return 1, is string without 'data scientist', return 0. If it is not string type, return 0, too.\n",
    "2. has_data_analyst, if the input value is a string, has 'data analyst' in it, return 1, is string without 'data analyst', return 0. If it is not string type, return 0, too.\n",
    "3. has_title(input_value, title_value), if the input_value has title_value, return 1, otherwise, return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_data_scientist(input_value):\n",
    "    target_string = 'data scientist'\n",
    "    if type(input_value) == str:\n",
    "        string_position = input_value.lower().find(target_string)\n",
    "        if string_position >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def has_data_analyst(input_value):\n",
    "    target_string = 'data analyst'\n",
    "    if type(input_value) == str:\n",
    "        string_position = input_value.lower().find(target_string)\n",
    "        if string_position >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def has_title(input_value, title_value):\n",
    "    if type(input_value) == str:\n",
    "        string_position = input_value.lower().find(title_value)\n",
    "        if string_position >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for testing case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_value = 'data scientist'\n",
    "test_bool = df_new.job_title[0:10].apply(has_title, args=(title_value,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     1\n",
      "2     1\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "7     1\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "12    0\n",
      "Name: job_title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personal_ID</th>\n",
       "      <th>job_ID</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>working_dates</th>\n",
       "      <th>job_description</th>\n",
       "      <th>skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Vice President / Fraud Data Scientist</td>\n",
       "      <td>JPMorgan Chase - Wilmington, DE</td>\n",
       "      <td>2015年3月 至 目前</td>\n",
       "      <td>Credit Card POS Transactional Fraud Acquisitio...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sr. Modeler / Data Scientist</td>\n",
       "      <td>American Eagle Outfitters - Pittsburgh, PA</td>\n",
       "      <td>2013年8月 至 2015年3月</td>\n",
       "      <td>Responsibilities Manage all customer modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Quantitative Engagement Manger</td>\n",
       "      <td>Essex Lake Group - New York, NY</td>\n",
       "      <td>2011年12月 至 2013年8月</td>\n",
       "      <td>Responsibilities Present results to executive ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Sr. Risk Analyst</td>\n",
       "      <td>Paypal - 上海市</td>\n",
       "      <td>2011年3月 至 2011年12月</td>\n",
       "      <td>Responsibilities Develop seller risk modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Junior Analytics Manager</td>\n",
       "      <td>Opera Solutions - 上海市</td>\n",
       "      <td>2007年7月 至 2011年3月</td>\n",
       "      <td>Responsibilities Present business insights to ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personal_ID  job_ID                              job_title  \\\n",
       "1            2       1  Vice President / Fraud Data Scientist   \n",
       "2            2       2           Sr. Modeler / Data Scientist   \n",
       "3            2       3         Quantitative Engagement Manger   \n",
       "4            2       4                       Sr. Risk Analyst   \n",
       "5            2       5               Junior Analytics Manager   \n",
       "\n",
       "                                 company_name       working_dates  \\\n",
       "1             JPMorgan Chase - Wilmington, DE        2015年3月 至 目前   \n",
       "2  American Eagle Outfitters - Pittsburgh, PA   2013年8月 至 2015年3月   \n",
       "3             Essex Lake Group - New York, NY  2011年12月 至 2013年8月   \n",
       "4                                Paypal - 上海市  2011年3月 至 2011年12月   \n",
       "5                       Opera Solutions - 上海市   2007年7月 至 2011年3月   \n",
       "\n",
       "                                     job_description skill  \n",
       "1  Credit Card POS Transactional Fraud Acquisitio...   NaN  \n",
       "2  Responsibilities Manage all customer modeling ...   NaN  \n",
       "3  Responsibilities Present results to executive ...   NaN  \n",
       "4  Responsibilities Develop seller risk modeling ...   NaN  \n",
       "5  Responsibilities Present business insights to ...   NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a series, with value 1 indicating job title has data scientist, 0 has job title, but not data scientist in it, 0 if it is  'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_value = 'data scientist'\n",
    "is_data_scientist = df_new.job_title.apply(has_title, args=(title_value,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how many titles have 'data scientist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7924"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(is_data_scientist ==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a series, with value 1 indicating job title has data analyst, 0 has job title, but no data analyst in it, 0 if it is  'NaN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_value = 'data analyst'\n",
    "is_data_analyst = df_new.job_title.apply(has_title, args=(title_value,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4383"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(is_data_analyst==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check whether a title has both data scientist and data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((is_data_scientist ==1)&(is_data_analyst==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_DS_DA=df_new.job_title[(is_data_scientist ==1)&(is_data_analyst==1)]\n",
    "len(both_DS_DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387    Business Data Analyst/Data Scientist/SME\n",
       "388    Business Data Analyst/Data Scientist/SME\n",
       "553                 Data Analyst/Data Scientist\n",
       "Name: job_title, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_DS_DA[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code below is for DataFrame series logic operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    0\n",
      "2    0\n",
      "dtype: int64\n",
      "0    1\n",
      "1    1\n",
      "2    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "a = pd.Series([1,0,0])\n",
    "b = pd.Series([1,1,0])\n",
    "c=a&b\n",
    "print (c)\n",
    "c= a|b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above is for DataFrame series logic operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total number 1: ', len(is_data_scientist), ' total number 2: ', len(is_data_analyst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total number 1: ', sum(is_data_scientist ==-2), ' total number 2: ', sum(is_data_analyst ==-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_work_experience = is_data_scientist.map(lambda x: 1 if x==-2 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Set is_data_analyst element value, if the value is 1, still 1, otherwise 0\n",
    "## 2. Set is_data_scientist element value, if the value is 1, still 1, otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data_analyst = is_data_analyst.map(lambda x: 1 if x==1 else 0)\n",
    "is_data_scientist = is_data_scientist.map(lambda x: 1 if x==1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(is_data_scientist.unique())\n",
    "print(is_data_analyst.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_title = (is_data_analyst|is_data_scientist)\n",
    "other_title = other_title.map(lambda x: 1 if x==0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new dataframe with extra columns: is_data_analyst, is_data_scientist, no_work_experience, and other_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yi1fe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personal_ID</th>\n",
       "      <th>job_ID</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>working_dates</th>\n",
       "      <th>job_description</th>\n",
       "      <th>skill</th>\n",
       "      <th>is_data_analyst</th>\n",
       "      <th>is_data_scientist</th>\n",
       "      <th>other_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Vice President / Fraud Data Scientist</td>\n",
       "      <td>JPMorgan Chase - Wilmington, DE</td>\n",
       "      <td>2015年3月 至 目前</td>\n",
       "      <td>Credit Card POS Transactional Fraud Acquisitio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sr. Modeler / Data Scientist</td>\n",
       "      <td>American Eagle Outfitters - Pittsburgh, PA</td>\n",
       "      <td>2013年8月 至 2015年3月</td>\n",
       "      <td>Responsibilities Manage all customer modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Quantitative Engagement Manger</td>\n",
       "      <td>Essex Lake Group - New York, NY</td>\n",
       "      <td>2011年12月 至 2013年8月</td>\n",
       "      <td>Responsibilities Present results to executive ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Sr. Risk Analyst</td>\n",
       "      <td>Paypal - 上海市</td>\n",
       "      <td>2011年3月 至 2011年12月</td>\n",
       "      <td>Responsibilities Develop seller risk modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Junior Analytics Manager</td>\n",
       "      <td>Opera Solutions - 上海市</td>\n",
       "      <td>2007年7月 至 2011年3月</td>\n",
       "      <td>Responsibilities Present business insights to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personal_ID  job_ID                              job_title  \\\n",
       "0            2       1  Vice President / Fraud Data Scientist   \n",
       "1            2       2           Sr. Modeler / Data Scientist   \n",
       "2            2       3         Quantitative Engagement Manger   \n",
       "3            2       4                       Sr. Risk Analyst   \n",
       "4            2       5               Junior Analytics Manager   \n",
       "\n",
       "                                 company_name       working_dates  \\\n",
       "0             JPMorgan Chase - Wilmington, DE        2015年3月 至 目前   \n",
       "1  American Eagle Outfitters - Pittsburgh, PA   2013年8月 至 2015年3月   \n",
       "2             Essex Lake Group - New York, NY  2011年12月 至 2013年8月   \n",
       "3                                Paypal - 上海市  2011年3月 至 2011年12月   \n",
       "4                       Opera Solutions - 上海市   2007年7月 至 2011年3月   \n",
       "\n",
       "                                     job_description skill  is_data_analyst  \\\n",
       "0  Credit Card POS Transactional Fraud Acquisitio...   NaN                0   \n",
       "1  Responsibilities Manage all customer modeling ...   NaN                0   \n",
       "2  Responsibilities Present results to executive ...   NaN                0   \n",
       "3  Responsibilities Develop seller risk modeling ...   NaN                0   \n",
       "4  Responsibilities Present business insights to ...   NaN                0   \n",
       "\n",
       "   is_data_scientist  other_title  \n",
       "0                  1            0  \n",
       "1                  1            0  \n",
       "2                  0            1  \n",
       "3                  0            1  \n",
       "4                  0            1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.loc[:, 'is_data_analyst'] = is_data_analyst\n",
    "df_new.loc[:, 'is_data_scientist'] = is_data_scientist\n",
    "#df_new['no_work_experience'] = no_work_experience\n",
    "df_new.loc[:, 'other_title'] = other_title\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yi1fe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personal_ID</th>\n",
       "      <th>job_ID</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>working_dates</th>\n",
       "      <th>job_description</th>\n",
       "      <th>skill</th>\n",
       "      <th>is_data_analyst</th>\n",
       "      <th>is_data_scientist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Vice President / Fraud Data Scientist</td>\n",
       "      <td>JPMorgan Chase - Wilmington, DE</td>\n",
       "      <td>2015年3月 至 目前</td>\n",
       "      <td>Credit Card POS Transactional Fraud Acquisitio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sr. Modeler / Data Scientist</td>\n",
       "      <td>American Eagle Outfitters - Pittsburgh, PA</td>\n",
       "      <td>2013年8月 至 2015年3月</td>\n",
       "      <td>Responsibilities Manage all customer modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Quantitative Engagement Manger</td>\n",
       "      <td>Essex Lake Group - New York, NY</td>\n",
       "      <td>2011年12月 至 2013年8月</td>\n",
       "      <td>Responsibilities Present results to executive ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Sr. Risk Analyst</td>\n",
       "      <td>Paypal - 上海市</td>\n",
       "      <td>2011年3月 至 2011年12月</td>\n",
       "      <td>Responsibilities Develop seller risk modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Junior Analytics Manager</td>\n",
       "      <td>Opera Solutions - 上海市</td>\n",
       "      <td>2007年7月 至 2011年3月</td>\n",
       "      <td>Responsibilities Present business insights to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personal_ID  job_ID                              job_title  \\\n",
       "0            2       1  Vice President / Fraud Data Scientist   \n",
       "1            2       2           Sr. Modeler / Data Scientist   \n",
       "2            2       3         Quantitative Engagement Manger   \n",
       "3            2       4                       Sr. Risk Analyst   \n",
       "4            2       5               Junior Analytics Manager   \n",
       "\n",
       "                                 company_name       working_dates  \\\n",
       "0             JPMorgan Chase - Wilmington, DE        2015年3月 至 目前   \n",
       "1  American Eagle Outfitters - Pittsburgh, PA   2013年8月 至 2015年3月   \n",
       "2             Essex Lake Group - New York, NY  2011年12月 至 2013年8月   \n",
       "3                                Paypal - 上海市  2011年3月 至 2011年12月   \n",
       "4                       Opera Solutions - 上海市   2007年7月 至 2011年3月   \n",
       "\n",
       "                                     job_description skill  is_data_analyst  \\\n",
       "0  Credit Card POS Transactional Fraud Acquisitio...   NaN                0   \n",
       "1  Responsibilities Manage all customer modeling ...   NaN                0   \n",
       "2  Responsibilities Present results to executive ...   NaN                0   \n",
       "3  Responsibilities Develop seller risk modeling ...   NaN                0   \n",
       "4  Responsibilities Present business insights to ...   NaN                0   \n",
       "\n",
       "   is_data_scientist  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.drop(['other_title'], axis=1, inplace=True)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yi1fe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\yi1fe\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personal_ID</th>\n",
       "      <th>job_ID</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>working_dates</th>\n",
       "      <th>job_description</th>\n",
       "      <th>skill</th>\n",
       "      <th>is_data_analyst</th>\n",
       "      <th>is_data_scientist</th>\n",
       "      <th>other_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Vice President / Fraud Data Scientist</td>\n",
       "      <td>JPMorgan Chase - Wilmington, DE</td>\n",
       "      <td>2015年3月 至 目前</td>\n",
       "      <td>Credit Card POS Transactional Fraud Acquisitio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sr. Modeler / Data Scientist</td>\n",
       "      <td>American Eagle Outfitters - Pittsburgh, PA</td>\n",
       "      <td>2013年8月 至 2015年3月</td>\n",
       "      <td>Responsibilities Manage all customer modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Quantitative Engagement Manger</td>\n",
       "      <td>Essex Lake Group - New York, NY</td>\n",
       "      <td>2011年12月 至 2013年8月</td>\n",
       "      <td>Responsibilities Present results to executive ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Sr. Risk Analyst</td>\n",
       "      <td>Paypal - 上海市</td>\n",
       "      <td>2011年3月 至 2011年12月</td>\n",
       "      <td>Responsibilities Develop seller risk modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Junior Analytics Manager</td>\n",
       "      <td>Opera Solutions - 上海市</td>\n",
       "      <td>2007年7月 至 2011年3月</td>\n",
       "      <td>Responsibilities Present business insights to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personal_ID  job_ID                              job_title  \\\n",
       "0            2       1  Vice President / Fraud Data Scientist   \n",
       "1            2       2           Sr. Modeler / Data Scientist   \n",
       "2            2       3         Quantitative Engagement Manger   \n",
       "3            2       4                       Sr. Risk Analyst   \n",
       "4            2       5               Junior Analytics Manager   \n",
       "\n",
       "                                 company_name       working_dates  \\\n",
       "0             JPMorgan Chase - Wilmington, DE        2015年3月 至 目前   \n",
       "1  American Eagle Outfitters - Pittsburgh, PA   2013年8月 至 2015年3月   \n",
       "2             Essex Lake Group - New York, NY  2011年12月 至 2013年8月   \n",
       "3                                Paypal - 上海市  2011年3月 至 2011年12月   \n",
       "4                       Opera Solutions - 上海市   2007年7月 至 2011年3月   \n",
       "\n",
       "                                     job_description skill  is_data_analyst  \\\n",
       "0  Credit Card POS Transactional Fraud Acquisitio...   NaN                0   \n",
       "1  Responsibilities Manage all customer modeling ...   NaN                0   \n",
       "2  Responsibilities Present results to executive ...   NaN                0   \n",
       "3  Responsibilities Develop seller risk modeling ...   NaN                0   \n",
       "4  Responsibilities Present business insights to ...   NaN                0   \n",
       "\n",
       "   is_data_scientist  other_title  \n",
       "0                  1            0  \n",
       "1                  1            0  \n",
       "2                  0            1  \n",
       "3                  0            1  \n",
       "4                  0            1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.loc[:, 'other_title'] = other_title\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new data frame, in which all record has work experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35537, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personal_ID</th>\n",
       "      <th>job_ID</th>\n",
       "      <th>job_title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>working_dates</th>\n",
       "      <th>job_description</th>\n",
       "      <th>skill</th>\n",
       "      <th>is_data_analyst</th>\n",
       "      <th>is_data_scientist</th>\n",
       "      <th>other_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Vice President / Fraud Data Scientist</td>\n",
       "      <td>JPMorgan Chase - Wilmington, DE</td>\n",
       "      <td>2015年3月 至 目前</td>\n",
       "      <td>Credit Card POS Transactional Fraud Acquisitio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sr. Modeler / Data Scientist</td>\n",
       "      <td>American Eagle Outfitters - Pittsburgh, PA</td>\n",
       "      <td>2013年8月 至 2015年3月</td>\n",
       "      <td>Responsibilities Manage all customer modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Quantitative Engagement Manger</td>\n",
       "      <td>Essex Lake Group - New York, NY</td>\n",
       "      <td>2011年12月 至 2013年8月</td>\n",
       "      <td>Responsibilities Present results to executive ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Sr. Risk Analyst</td>\n",
       "      <td>Paypal - 上海市</td>\n",
       "      <td>2011年3月 至 2011年12月</td>\n",
       "      <td>Responsibilities Develop seller risk modeling ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Junior Analytics Manager</td>\n",
       "      <td>Opera Solutions - 上海市</td>\n",
       "      <td>2007年7月 至 2011年3月</td>\n",
       "      <td>Responsibilities Present business insights to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personal_ID  job_ID                              job_title  \\\n",
       "0            2       1  Vice President / Fraud Data Scientist   \n",
       "1            2       2           Sr. Modeler / Data Scientist   \n",
       "2            2       3         Quantitative Engagement Manger   \n",
       "3            2       4                       Sr. Risk Analyst   \n",
       "4            2       5               Junior Analytics Manager   \n",
       "\n",
       "                                 company_name       working_dates  \\\n",
       "0             JPMorgan Chase - Wilmington, DE        2015年3月 至 目前   \n",
       "1  American Eagle Outfitters - Pittsburgh, PA   2013年8月 至 2015年3月   \n",
       "2             Essex Lake Group - New York, NY  2011年12月 至 2013年8月   \n",
       "3                                Paypal - 上海市  2011年3月 至 2011年12月   \n",
       "4                       Opera Solutions - 上海市   2007年7月 至 2011年3月   \n",
       "\n",
       "                                     job_description skill  is_data_analyst  \\\n",
       "0  Credit Card POS Transactional Fraud Acquisitio...   NaN                0   \n",
       "1  Responsibilities Manage all customer modeling ...   NaN                0   \n",
       "2  Responsibilities Present results to executive ...   NaN                0   \n",
       "3  Responsibilities Develop seller risk modeling ...   NaN                0   \n",
       "4  Responsibilities Present business insights to ...   NaN                0   \n",
       "\n",
       "   is_data_scientist  other_title  \n",
       "0                  1            0  \n",
       "1                  1            0  \n",
       "2                  0            1  \n",
       "3                  0            1  \n",
       "4                  0            1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_has_title = df_new.copy()\n",
    "df_has_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35537, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_has_title.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We finish data pre-processing, now we begin to do model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">1. Model building</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code below is for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_vectorizer = CountVectorizer()\n",
    "X = CT_vectorizer.fit_transform(df_has_title.job_description[0:2])\n",
    "print(CT_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CT_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vocabulary = {'credit': 0, 'models':1}\n",
    "CT_vectorizer = CountVectorizer(vocabulary=test_vocabulary)\n",
    "X = CT_vectorizer.fit_transform(df_has_title.job_description[0:2])\n",
    "print(CT_vectorizer.get_feature_names())\n",
    "\n",
    "CT_vectorizer = CountVectorizer()\n",
    "corpus = [ 'This is the first document.',\n",
    "           'This document is the second document.',\n",
    "           'And this is the third one.',\n",
    "           'Is this the first document?']\n",
    "X = CT_vectorizer.fit_transform(corpus)\n",
    "print(CT_vectorizer.vocabulary_)\n",
    "\n",
    "CT_vectorizer = CountVectorizer(max_features =1)\n",
    "X = CT_vectorizer.fit_transform(corpus)\n",
    "print(CT_vectorizer.vocabulary_)\n",
    "\n",
    "CT_vectorizer = CountVectorizer(max_features =4)\n",
    "corpus = [ 'This is the first document. This is good',\n",
    "           'This document is the second document. How many document',\n",
    "           'And this is the third one. third, third, third, third ',\n",
    "           'Is this the first document?']\n",
    "X = CT_vectorizer.fit_transform(corpus)\n",
    "print(CT_vectorizer.vocabulary_)\n",
    "\n",
    "print(X)\n",
    "\n",
    "CT_vectorizer = CountVectorizer()\n",
    "corpus = [ 'This is the first document. This is good',\n",
    "           'This document is the second document. How many document',\n",
    "           'And this is the third one. third, third, third, third ',\n",
    "           'Is this the first document?']\n",
    "X = CT_vectorizer.fit_transform(corpus)\n",
    "print(CT_vectorizer.vocabulary_)\n",
    "print(X)\n",
    "\n",
    "CT_vectorizer = CountVectorizer(binary=True)\n",
    "X = CT_vectorizer.fit_transform(corpus)\n",
    "print(CT_vectorizer.vocabulary_)\n",
    "print(X)\n",
    "\n",
    "CT_vectorizer = CountVectorizer(max_df = 3)\n",
    "X = CT_vectorizer.fit_transform(corpus)\n",
    "print(CT_vectorizer.vocabulary_)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code above is for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cross-validation and AUC score to tune hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CountVectorizer and LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_vectorizer = CountVectorizer()\n",
    "Mul_NB = MultinomialNB()\n",
    "Log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model is for data analyst prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CountVectorizer() default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =[('Count_vectorizer', CountVectorizer()),\n",
    "        ('Logistic_R', LogisticRegression())]\n",
    "model_1=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use balanced data in logistic regression and token pattern must begin with letter, stop_words ='english', ngram uses default value, binary=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('Count_vectorizer', CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in regular expression \\d means digit from 0 to 9\n",
    "#\\W Matches any non-word character\n",
    "#^ y default, the match must start at the beginning of the string; in multiline mode, \n",
    "#it must start at the beginning of the line.\n",
    "#Typing a caret ^ after the opening square bracket negates the character class. \n",
    "#The result is that the character class matches any character that is not in the character class\n",
    "model_1.set_params(Count_vectorizer__binary =True, Count_vectorizer__stop_words ='english',\n",
    "                   Count_vectorizer__token_pattern =r'\\b[a-zA-Z][^\\d\\W]+\\b',\n",
    "                   Logistic_R__class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='\\\\b[a-zA-Z][^\\\\d\\\\W]+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(model_1.steps[0][1])\n",
    "print(model_1.steps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_valid_proba =np.zeros((df_has_title.job_description.size,2))\n",
    "for train_index, valid_index in kf.split(df_has_title.job_description):\n",
    "    X_train, X_valid = df_has_title.job_description[train_index], df_has_title.job_description[valid_index]\n",
    "    y_train, y_valid = df_has_title.is_data_analyst[train_index], df_has_title.is_data_analyst[valid_index]\n",
    "    model_1.fit(X_train, y_train)\n",
    "    pred_y_valid_proba[valid_index] = model_1.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8380563214767978]\n"
     ]
    }
   ],
   "source": [
    "auc_area =[]\n",
    "auc_area.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))\n",
    "print(auc_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Except set ngram_range to (1,2), other parameters in model_2 is the same as model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('Count_vectorizer', CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.set_params(Count_vectorizer__binary =True, \n",
    "                   Count_vectorizer__ngram_range=(1,2),\n",
    "                   Count_vectorizer__token_pattern =r'\\b[a-zA-Z][^\\d\\W]+\\b',\n",
    "                   Count_vectorizer__stop_words ='english',\n",
    "                   Logistic_R__class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='\\\\b[a-zA-Z][^\\\\d\\\\W]+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(model_2.steps[0][1])\n",
    "print(model_2.steps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_valid_proba =np.zeros((df_has_title.job_description.size,2))\n",
    "for train_index, valid_index in kf.split(df_has_title.job_description):\n",
    "    X_train, X_valid = df_has_title.job_description[train_index], df_has_title.job_description[valid_index]\n",
    "    y_train, y_valid = df_has_title.is_data_analyst[train_index], df_has_title.is_data_analyst[valid_index]\n",
    "    model_2.fit(X_train, y_train)\n",
    "    pred_y_valid_proba[valid_index] = model_2.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8380563214767978, 0.870015061811752]\n"
     ]
    }
   ],
   "source": [
    "auc_area.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))\n",
    "print(auc_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use balanced data in logistic regression and token pattern must begin with letter, stop_words ='english', ngram uses default value, binary=False, detault value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('Count_vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='englis...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3=Pipeline(steps)\n",
    "model_3.set_params(Count_vectorizer__binary =False, \n",
    "                   Count_vectorizer__ngram_range=(1,1),\n",
    "                   Count_vectorizer__token_pattern =r'\\b[a-zA-Z][^\\d\\W]+\\b',\n",
    "                   Count_vectorizer__stop_words ='english',\n",
    "                   Logistic_R__class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='\\\\b[a-zA-Z][^\\\\d\\\\W]+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(model_3.steps[0][1])\n",
    "print(model_3.steps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "steps =[('Count_vectorizer', CountVectorizer()),\n",
    "        ('Logistic_R', LogisticRegression())]\n",
    "model_2=Pipeline(steps)\n",
    "print(model_2.steps[0][1])\n",
    "print(model_2.steps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_valid_proba =np.zeros((df_has_title.job_description.size,2))\n",
    "for train_index, valid_index in kf.split(df_has_title.job_description):\n",
    "    X_train, X_valid = df_has_title.job_description[train_index], df_has_title.job_description[valid_index]\n",
    "    y_train, y_valid = df_has_title.is_data_analyst[train_index], df_has_title.is_data_analyst[valid_index]\n",
    "    model_3.fit(X_train, y_train)\n",
    "    pred_y_valid_proba[valid_index] = model_3.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.870015061811752"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_area.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8380563214767978, 0.870015061811752, 0.8305803852890334]\n"
     ]
    }
   ],
   "source": [
    "auc_area.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))\n",
    "print(auc_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use default value in weight in logistic regression and token pattern must begin with letter, stop_words ='english', ngram uses default value, binary=False, detault value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =[('Count_vectorizer', CountVectorizer()),\n",
    "        ('Logistic_R', LogisticRegression())]\n",
    "model_4=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='\\\\b[a-zA-Z][^\\\\d\\\\W]+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "model_4.set_params(Count_vectorizer__token_pattern =r'\\b[a-zA-Z][^\\d\\W]+\\b',\n",
    "                   Count_vectorizer__stop_words ='english')\n",
    "print(model_4.steps[0][1])\n",
    "print(model_4.steps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_valid_proba =np.zeros((df_has_title.job_description.size,2))\n",
    "for train_index, valid_index in kf.split(df_has_title.job_description):\n",
    "    X_train, X_valid = df_has_title.job_description[train_index], df_has_title.job_description[valid_index]\n",
    "    y_train, y_valid = df_has_title.is_data_analyst[train_index], df_has_title.is_data_analyst[valid_index]\n",
    "    model_4.fit(X_train, y_train)\n",
    "    pred_y_valid_proba[valid_index] = model_4.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8380563214767978, 0.870015061811752, 0.8305803852890334, 0.8351501489051665]\n"
     ]
    }
   ],
   "source": [
    "auc_area.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))\n",
    "print(auc_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting binary=True, ngram=(1,2) in CountVectorizer does help in improving the performance, also set weight as default value in LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.fit(df_has_title.job_description, df_has_title.is_data_analyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_2.steps[0][1].vocabulary_))\n",
    "print(model_2.steps[1][1].coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Switch_key_value(Input_dict):\n",
    "    #This function switch the key and value\n",
    "    #of the Input_dict\n",
    "    #The input_dict values are unique\n",
    "    #return Output_dict, in which\n",
    "    #the key is the value in Input_dict\n",
    "    #and the value is the key in Input_dict\n",
    "    Output_dict ={}\n",
    "    for key in Input_dict:\n",
    "        Output_dict[Input_dict[key]] = key\n",
    "        \n",
    "    return Output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test case for function Switch_key_value\n",
    "Input_dict = {'a': 1, 'b':2}\n",
    "result_dict = Switch_key_value(Input_dict)\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = range(len(model_2.steps[0][1].vocabulary_))\n",
    "type(vocabulary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_list is a list of tuple, the element is (i, coef[i])\n",
    "feature_list =[]\n",
    "for i in vocabulary_list:\n",
    "    feature_list.append((i, model_2.steps[1][1].coef_[0,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_list[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFun(c):\n",
    "    return c[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_list = sorted(feature_list, key=myFun, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_importance_list[0][1])\n",
    "print(max(model_2.steps[1][1].coef_[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_importance_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_switch = Switch_key_value(model_2.steps[0][1].vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    print(vocabulary_switch[i], end ='# ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,50):\n",
    "    print(vocabulary_switch[feature_importance_list[i][0]], end='# ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Find so many non-meaningful numbers in the extracted vocabulary,  may need to use different token pattern to only consider the words which does not have number(s) in it</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =[('Count_vectorizer', CountVectorizer()),\n",
    "        ('Logistic_R', LogisticRegression())]\n",
    "model=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(Count_vectorizer__token_pattern=r'\\b[a-zA-Z][^\\d\\W]+\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.steps[0][1])\n",
    "print(model.steps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_valid_proba =np.zeros((df_has_title.job_description.size,2))\n",
    "for train_index, valid_index in kf.split(df_has_title.job_description):\n",
    "    X_train, X_valid = df_has_title.job_description[train_index], df_has_title.job_description[valid_index]\n",
    "    y_train, y_valid = df_has_title.is_data_analyst[train_index], df_has_title.is_data_analyst[valid_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_y_valid_proba[valid_index] = model.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_area.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))\n",
    "print(auc_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(df_has_title.job_description, df_has_title.is_data_analyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = range(len(model.steps[0][1].vocabulary_))\n",
    "feature_list =[]\n",
    "for i in vocabulary_list:\n",
    "    feature_list.append((i, model.steps[1][1].coef_[0,i]))\n",
    "feature_importance_list = sorted(feature_list, key=myFun, reverse=True)\n",
    "print(feature_importance_list[0][1])\n",
    "print(max(model.steps[1][1].coef_[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_switch = Switch_key_value(model.steps[0][1].vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    print(vocabulary_switch[i], end =' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,50):\n",
    "    print(vocabulary_switch[feature_importance_list[i][0]], end='# ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set binary =True to see if there is any improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =[('Count_vectorizer', CountVectorizer()),\n",
    "        ('Logistic_R', LogisticRegression())]\n",
    "model_1=Pipeline(steps)\n",
    "model_1.set_params(Count_vectorizer__binary =True, \n",
    "                   Count_vectorizer__token_pattern=r'\\b[a-zA-Z][^\\d\\W]+\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_1.steps[0][1])\n",
    "print(model_1.steps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_valid_proba =np.zeros((df_has_title.job_description.size,2))\n",
    "for train_index, valid_index in kf.split(df_has_title.job_description):\n",
    "    X_train, X_valid = df_has_title.job_description[train_index], df_has_title.job_description[valid_index]\n",
    "    y_train, y_valid = df_has_title.is_data_analyst[train_index], df_has_title.is_data_analyst[valid_index]\n",
    "    model_1.fit(X_train, y_train)\n",
    "    pred_y_valid_proba[valid_index] = model_1.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_area.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))\n",
    "print(auc_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.fit(df_has_title.job_description, df_has_title.is_data_analyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_list = range(len(model_1.steps[0][1].vocabulary_))\n",
    "#feature_list is a list of tuple, the element is (i, coef[i])\n",
    "feature_list =[]\n",
    "for i in vocabulary_list:\n",
    "    feature_list.append((i, model_1.steps[1][1].coef_[0,i]))\n",
    "feature_importance_list = sorted(feature_list, key=myFun, reverse=True)\n",
    "vocabulary_switch = Switch_key_value(model_1.steps[0][1].vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_switch = Switch_key_value(model_1.steps[0][1].vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    print(vocabulary_switch[i], end ='# ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,50):\n",
    "    print(vocabulary_switch[feature_importance_list[i][0]], end='# ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider the weight = balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =[('Count_vectorizer', CountVectorizer()),\n",
    "        ('Logistic_R', LogisticRegression())]\n",
    "model_2=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.set_params(Count_vectorizer__binary =True, \n",
    "                   Count_vectorizer__token_pattern =r'\\b[a-zA-Z][^\\d\\W]+\\b',\n",
    "                   Logistic_R__class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider  ngram_range(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_1\n",
    "steps =[('Count_vectorizer', CountVectorizer()),\n",
    "        ('Logistic_R', LogisticRegression())]\n",
    "model_1 = Pipeline(steps)\n",
    "model_1.set_params(Count_vectorizer__binary =True, Count_vectorizer__ngram_range=(1,2),\n",
    "                 Count_vectorizer__token_pattern =r'\\b[a-zA-Z][^\\d\\W]+\\b',\n",
    "                 Logistic_R__class_weight = 'balanced')\n",
    "\n",
    "print(model_1.steps[0][1])\n",
    "print('********************')\n",
    "print(model_1.steps[1][1])\n",
    "\n",
    "pred_y_valid_proba =np.zeros((df_has_title.job_description.size,2))\n",
    "for train_index, valid_index in kf.split(df_has_title.job_description):\n",
    "    X_train, X_valid = df_has_title.job_description[train_index], df_has_title.job_description[valid_index]\n",
    "    y_train, y_valid = df_has_title.is_data_analyst[train_index], df_has_title.is_data_analyst[valid_index]\n",
    "    model_1.fit(X_train, y_train)\n",
    "    pred_y_valid_proba[valid_index] = model.predict_proba(X_valid)\n",
    "\n",
    "auc_area =[]\n",
    "auc_area.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))\n",
    "print(auc_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_area.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))\n",
    "print(auc_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider df_max in the model below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df_list=range(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in max_df_list:\n",
    "    max_df_value = 0.1*i\n",
    "    model.set_params(Count_vectorizer__binary =True, Count_vectorizer__max_df = max_df_value)\n",
    "    pred_y_valid_proba =np.zeros((df_has_title.job_description.size,2))\n",
    "    for train_index, valid_index in kf.split(df_has_title.job_description):\n",
    "        X_train, X_valid = df_has_title.job_description[train_index], df_has_title.job_description[valid_index]\n",
    "        y_train, y_valid = df_has_title.is_data_analyst[train_index], df_has_title.is_data_analyst[valid_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        pred_y_valid_proba[valid_index] = model.predict_proba(X_valid)\n",
    "    auc_area.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(Count_vectorizer__binary =True, Count_vectorizer__max_df = 1.0,\n",
    "                 Count_vectorizer__ngram_range = (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_valid_proba =np.zeros((df_has_title.job_description.size,2))\n",
    "for train_index, valid_index in kf.split(df_has_title.job_description):\n",
    "    X_train, X_valid = df_has_title.job_description[train_index], df_has_title.job_description[valid_index]\n",
    "    y_train, y_valid = df_has_title.is_data_analyst[train_index], df_has_title.is_data_analyst[valid_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_y_valid_proba[valid_index] = model.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_area.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))\n",
    "print(auc_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use TfidfVectorizer + Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfid_vectorizer = TfidfVectorizer()\n",
    "Log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#at first, use default setting\n",
    "steps =[('Tfidf_Vectorizer', TfidfVectorizer()),\n",
    "        ('Logistic_R', Log_reg)]\n",
    "model=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_valid_proba =np.zeros((df_has_title.job_description.size,2))\n",
    "for train_index, valid_index in kf.split(df_has_title.job_description):\n",
    "    X_train, X_valid = df_has_title.job_description[train_index], df_has_title.job_description[valid_index]\n",
    "    y_train, y_valid = df_has_title.is_data_analyst[train_index], df_has_title.is_data_analyst[valid_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    pred_y_valid_proba[valid_index] = model.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_area_Tfid =[]\n",
    "auc_area_Tfid.append(roc_auc_score(df_has_title.is_data_analyst, pred_y_valid_proba[:,1]))\n",
    "print(auc_area_Tfid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into two parts: train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This model is for data analyst prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_has_title, df_has_title.is_data_analyst,\n",
    "                                                    test_size=0.2, random_state=42,\n",
    "                                                    stratify = df_has_title.is_data_analyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(X_train.is_data_analyst ==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop =True, inplace =True)\n",
    "X_test.reset_index(drop =True, inplace =True)\n",
    "y_train.reset_index(drop =True, inplace =True)\n",
    "y_test.reset_index(drop = True, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_vectorizer = CountVectorizer()\n",
    "Mul_NB = MultinomialNB()\n",
    "Log_reg = LogisticRegression()\n",
    "steps =[('Count_vectorizer', CT_vectorizer),\n",
    "        ('Multi_NB', Mul_NB)]\n",
    "model=Pipeline(steps)\n",
    "model.fit(X_train.job_description, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_train = model.predict(X_train.job_description)\n",
    "pred_y_test = model.predict(X_test.job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use CountVectorizer and MultinomialNB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "Gaussian_NB = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class To_dense_Tfid_vectorizer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.vectorizer.fit(X)\n",
    "        \n",
    "    def transform(self, X, y =None):\n",
    "        new_array = self.vectorizer.transform(X)\n",
    "        new_array = new_array.toarray()\n",
    "        return new_array\n",
    "    \n",
    "    def fit_transform(self, X, y =None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =[('Dense_after_TF', To_dense_Tfid_vectorizer()),\n",
    "        ('G_NB', Gaussian_NB)]\n",
    "model=Pipeline(steps)\n",
    "model.fit(X_train.job_description, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CT_vectorizer = CountVectorizer()\n",
    "#Mul_NB = MultinomialNB()\n",
    "#Log_reg = LogisticRegression()\n",
    "steps =[('Count_vectorizer', CT_vectorizer),\n",
    "        ('Logistic_R', Log_reg)]\n",
    "model=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_train = model.predict(X_train.job_description)\n",
    "pred_y_test = model.predict(X_test.job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use count_vectorizer and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =[('Tfidf_Vectorizer', TfidfVectorizer()),\n",
    "        ('Logistic_R', Log_reg)]\n",
    "model=Pipeline(steps)\n",
    "model.fit(X_train.job_description, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_train = model.predict(X_train.job_description)\n",
    "pred_y_test = model.predict(X_test.job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TfidVectorizer and logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use <span style=\"color:red\">(CountVectorizer + logistic regression) </span> has better recall score and f1 score for class 1 (is data analyst).    Use <span style=\"color:red\">(TfidfVectorizer + logistic regression) </span> has better precision score for class 1 (is data analyst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This model is for data scientist prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_has_title, df_has_title.is_data_scientist,\n",
    "                                                    test_size=0.2, random_state=42,\n",
    "                                                    stratify = df_has_title.is_data_scientist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop =True, inplace =True)\n",
    "X_test.reset_index(drop =True, inplace =True)\n",
    "y_train.reset_index(drop =True, inplace =True)\n",
    "y_test.reset_index(drop = True, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =[('Count_vectorizer', CT_vectorizer),\n",
    "        ('Logistic_R', Log_reg)]\n",
    "model=Pipeline(steps)\n",
    "model.set_params(Logistic_R__max_iter = 1000)\n",
    "model.fit(X_train.job_description, y_train)\n",
    "pred_y_train = model.predict(X_train.job_description)\n",
    "pred_y_test = model.predict(X_test.job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_y_train))\n",
    "print(classification_report(y_test, pred_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try strong regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps =[('Count_vectorizer', CT_vectorizer),\n",
    "        ('Logistic_R', Log_reg)]\n",
    "model=Pipeline(steps)\n",
    "model.set_params(Logistic_R__max_iter = 1000, Logistic_R__C=0.1)\n",
    "model.fit(X_train.job_description, y_train)\n",
    "pred_y_train = model.predict(X_train.job_description)\n",
    "pred_y_test = model.predict(X_test.job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, pred_y_train))\n",
    "print(classification_report(y_test, pred_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
