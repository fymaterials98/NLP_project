{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 150000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 200 # max number of words in a comment to use\n",
    "\n",
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/data_data_scientist.csv')\n",
    "df2 = pd.read_csv('./data/data_engineer.csv', nrows =25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.concat([df1, df2], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop =True, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>additional_information</th>\n",
       "      <th>job_ID</th>\n",
       "      <th>job_company</th>\n",
       "      <th>job_date</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_title</th>\n",
       "      <th>person_ID</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Core CompetenciesData Modeling &amp; Visualization...</td>\n",
       "      <td>1</td>\n",
       "      <td>TRILLO INC - San Francisco Bay Area, CA</td>\n",
       "      <td>January 2018 to Present</td>\n",
       "      <td>* Incorporate Machine Learning solutions to fa...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>1</td>\n",
       "      <td>CLUSTERING (Less than 1 year), DATA MINING (Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TECHNICAL EXPERTISEStatistics/Machine Learning...</td>\n",
       "      <td>1</td>\n",
       "      <td>Pepsico - Plano, TX</td>\n",
       "      <td>September 2018 to Present</td>\n",
       "      <td>Fleet Analysis: Exploring the Truck repair and...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Hadoop (Less than 1 year), PYTHON (Less than 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TECHNICAL EXPERTISEStatistics/Machine Learning...</td>\n",
       "      <td>2</td>\n",
       "      <td>CISCO - San Jose, CA</td>\n",
       "      <td>May 2018 to July 2018</td>\n",
       "      <td>Developed Python scripts to automate the load...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Hadoop (Less than 1 year), PYTHON (Less than 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TECHNICAL EXPERTISEStatistics/Machine Learning...</td>\n",
       "      <td>3</td>\n",
       "      <td>PHOTON INFOTECH - Bohemia, NY</td>\n",
       "      <td>September 2017 to March 2018</td>\n",
       "      <td>Developed a personalized recommender engine fo...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Hadoop (Less than 1 year), PYTHON (Less than 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TECHNICAL EXPERTISEStatistics/Machine Learning...</td>\n",
       "      <td>4</td>\n",
       "      <td>APPLE INC - Cupertino, CA</td>\n",
       "      <td>January 2016 to August 2017</td>\n",
       "      <td>Played a key role in developing and maintainin...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>Hadoop (Less than 1 year), PYTHON (Less than 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              additional_information  job_ID  \\\n",
       "0  Core CompetenciesData Modeling & Visualization...       1   \n",
       "1  TECHNICAL EXPERTISEStatistics/Machine Learning...       1   \n",
       "2  TECHNICAL EXPERTISEStatistics/Machine Learning...       2   \n",
       "3  TECHNICAL EXPERTISEStatistics/Machine Learning...       3   \n",
       "4  TECHNICAL EXPERTISEStatistics/Machine Learning...       4   \n",
       "\n",
       "                               job_company                      job_date  \\\n",
       "0  TRILLO INC - San Francisco Bay Area, CA       January 2018 to Present   \n",
       "1                      Pepsico - Plano, TX     September 2018 to Present   \n",
       "2                     CISCO - San Jose, CA         May 2018 to July 2018   \n",
       "3            PHOTON INFOTECH - Bohemia, NY  September 2017 to March 2018   \n",
       "4                APPLE INC - Cupertino, CA   January 2016 to August 2017   \n",
       "\n",
       "                                     job_description       job_title  \\\n",
       "0  * Incorporate Machine Learning solutions to fa...  Data Scientist   \n",
       "1  Fleet Analysis: Exploring the Truck repair and...  Data Scientist   \n",
       "2   Developed Python scripts to automate the load...  Data Scientist   \n",
       "3  Developed a personalized recommender engine fo...  Data Scientist   \n",
       "4  Played a key role in developing and maintainin...  Data Scientist   \n",
       "\n",
       "   person_ID                                             skills  \n",
       "0          1  CLUSTERING (Less than 1 year), DATA MINING (Le...  \n",
       "1          2  Hadoop (Less than 1 year), PYTHON (Less than 1...  \n",
       "2          2  Hadoop (Less than 1 year), PYTHON (Less than 1...  \n",
       "3          2  Hadoop (Less than 1 year), PYTHON (Less than 1...  \n",
       "4          2  Hadoop (Less than 1 year), PYTHON (Less than 1...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_scientist(title_value):\n",
    "    target = 'data scientist'\n",
    "    if type(title_value) == float:\n",
    "        return -1\n",
    "    elif type(title_value) ==str:\n",
    "        if title_value.lower().find(target) >=0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data_scientist = df.job_title.map(find_data_scientist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1858"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(is_data_scientist==-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_scientist_binary(title_value):\n",
    "    target = 'data scientist'\n",
    "    if type(title_value) == float:\n",
    "        return 0\n",
    "    elif type(title_value) ==str:\n",
    "        if title_value.lower().find(target) >=0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_data_scientist = df.job_title.map(find_data_scientist_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(is_data_scientist==-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add is_data_scientist column to indicate whether the title has data scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_data_scientist']=is_data_scientist\n",
    "df_new = df.dropna(subset =['job_description'])\n",
    "df_new.reset_index(drop =True, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field, job_label):\n",
    "    text_first =df[text_field].map(lambda x: x.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \"))\n",
    "    text_second = text_first.map(lambda x: x.lower())\n",
    "    df_copy = pd.concat([text_second, df[job_label]], axis =1)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_description</th>\n",
       "      <th>is_data_scientist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* incorporate machine learning solutions to fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fleet analysis: exploring the truck repair and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>developed python scripts to automate the load...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>developed a personalized recommender engine fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>played a key role in developing and maintainin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     job_description  is_data_scientist\n",
       "0  * incorporate machine learning solutions to fa...                  1\n",
       "1  fleet analysis: exploring the truck repair and...                  1\n",
       "2   developed python scripts to automate the load...                  1\n",
       "3  developed a personalized recommender engine fo...                  1\n",
       "4  played a key role in developing and maintainin...                  1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comments =standardize_text(df_new, 'job_description', 'is_data_scientist')\n",
    "train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_description</th>\n",
       "      <th>is_data_scientist</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>* incorporate machine learning solutions to fa...</td>\n",
       "      <td>1</td>\n",
       "      <td>[incorporate, machine, learning, solutions, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fleet analysis: exploring the truck repair and...</td>\n",
       "      <td>1</td>\n",
       "      <td>[fleet, analysis, exploring, truck, repair, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>developed python scripts to automate the load...</td>\n",
       "      <td>1</td>\n",
       "      <td>[developed, python, scripts, automate, loading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>developed a personalized recommender engine fo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[developed, personalized, recommender, engine,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>played a key role in developing and maintainin...</td>\n",
       "      <td>1</td>\n",
       "      <td>[played, key, role, developing, maintaining, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     job_description  is_data_scientist  \\\n",
       "0  * incorporate machine learning solutions to fa...                  1   \n",
       "1  fleet analysis: exploring the truck repair and...                  1   \n",
       "2   developed python scripts to automate the load...                  1   \n",
       "3  developed a personalized recommender engine fo...                  1   \n",
       "4  played a key role in developing and maintainin...                  1   \n",
       "\n",
       "                                              tokens  \n",
       "0  [incorporate, machine, learning, solutions, fa...  \n",
       "1  [fleet, analysis, exploring, truck, repair, ma...  \n",
       "2  [developed, python, scripts, automate, loading...  \n",
       "3  [developed, personalized, recommender, engine,...  \n",
       "4  [played, key, role, developing, maintaining, s...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "train_comments['job_description'] = train_comments['job_description'].astype('str') \n",
    "#clean_train_comments.dtypes\n",
    "train_comments[\"tokens\"] = train_comments[\"job_description\"].apply(tokenizer.tokenize)\n",
    "# delete Stop Words\n",
    "train_comments[\"tokens\"] = train_comments[\"tokens\"].apply(lambda vec: [word for word in vec if word not in stop_words])\n",
    "   \n",
    "train_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3928510 words total, with a vocabulary size of 94918\n",
      "Max sentence length is 2846\n"
     ]
    }
   ],
   "source": [
    "all_training_words = [word for tokens in train_comments[\"tokens\"] for word in tokens]\n",
    "training_sentence_lengths = [len(tokens) for tokens in train_comments[\"tokens\"]]\n",
    "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(training_sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 94918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_path = \"./data/GoogleNews-vectors-negative300.bin.gz\"\n",
    "#Due to memory limit, load first frequent 300000 words\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 106070 unique tokens.\n",
      "(106071, 300)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(train_comments[\"job_description\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(train_comments[\"job_description\"].tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_GRU_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "#Why has shape len(train_word_index)+1\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_GRU():\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "    x = Embedding(len(train_word_index)+1, EMBEDDING_DIM, weights=[train_embedding_weights])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU = get_model_GRU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_GRU_data\n",
    "y_train = train_comments['is_data_scientist'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comments['is_data_scientist'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44434 samples, validate on 2339 samples\n",
      "Epoch 1/2\n",
      " - 768s - loss: 0.2665 - acc: 0.8907 - val_loss: 0.2006 - val_acc: 0.9124\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.946939 \n",
      "\n",
      "Epoch 2/2\n",
      " - 747s - loss: 0.1732 - acc: 0.9255 - val_loss: 0.1872 - val_acc: 0.9201\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.954993 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist = model_GRU.fit(X_tra, y_tra, batch_size=batch_size, epochs=num_epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU.save('./data/GRU_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Now try LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_LSTM():\n",
    "    inp = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "    x = Embedding(len(train_word_index)+1, EMBEDDING_DIM, weights=[train_embedding_weights])(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM = get_model_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44434 samples, validate on 2339 samples\n",
      "Epoch 1/2\n",
      " - 684s - loss: 0.2622 - acc: 0.8933 - val_loss: 0.2233 - val_acc: 0.9085\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.946409 \n",
      "\n",
      "Epoch 2/2\n",
      " - 683s - loss: 0.1748 - acc: 0.9251 - val_loss: 0.1877 - val_acc: 0.9213\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.953895 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hist_LSTM = model_LSTM.fit(X_tra, y_tra, batch_size=batch_size, epochs=num_epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.save('./data/LSTM_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
