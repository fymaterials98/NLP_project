{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "#driver = webdriver.Chrome(chromedriver)\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#head = web link\n",
    "driver.get(head)\n",
    "page = driver.page_source\n",
    "soup = BeautifulSoup(page,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Scraper_web(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def Input_Search(self, my_driver, what_content, where_content):\n",
    "        #my_driver, chrome web driver, alreay open the current page\n",
    "        #what_content, the input to \"what\" field, type: string\n",
    "        #where_content, the input to 'where\" field type: string\n",
    "        self.driver = my_driver\n",
    "        \n",
    "        #input what_content and where_content\n",
    "        search_title = self.driver.find_element_by_xpath(\"//input[@id = 'input-q']\")\n",
    "        search_title.clear()\n",
    "        search_title.send_keys(what_content)\n",
    "        \n",
    "        search_location = self.driver.find_element_by_xpath(\"//input[@id = 'input-l']\")\n",
    "        search_location.clear()\n",
    "        search_location.send_keys(where_content)\n",
    "        \n",
    "        find_resume_button = self.driver.find_element_by_xpath(\"//button[text() = 'Find resumes']\")\n",
    "        find_resume_button.click()\n",
    "        self.pre_address = self.driver.current_url\n",
    "        \n",
    "    def _Next_page(self):\n",
    "        try:\n",
    "            \n",
    "            next_button = self.driver.find_element_by_class_name('rezemp-pagination-nextbutton')\n",
    "            #print(next_button.text)    #for test\n",
    "            #next_button.click()\n",
    "            #print(self.driver.current_url)\n",
    "            #print('*******')\n",
    "            has_error_message = self.driver.find_element_by_xpath(\"//span[@class = 'icl-Alert-headline']\")\n",
    "            if (has_error_message.text == 'Error'):\n",
    "                print('page has error information')\n",
    "                return False\n",
    "            else:\n",
    "                self.driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "                return self.driver.page_source\n",
    "        except:\n",
    "            print('No next element')\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def _Current_page(self, add_address=''):\n",
    "        #add_page = '&start=' + str(50)\n",
    "        #pre_address = driver.current_url\n",
    "        current_address = self.pre_address + add_address\n",
    "        #print(current_address)\n",
    "        if (add_address !=''):\n",
    "            self.driver.get(current_address)\n",
    "            \n",
    "        return self.driver.page_source\n",
    "        \n",
    "    def Find_Links(self, current_page):\n",
    "        #current_page is the current page source code\n",
    "        #page = driver.page_source\n",
    "        soup = BeautifulSoup(current_page,'html.parser')\n",
    "        find_resume = soup.find_all(\"div\", class_=\"rezemp-ResumeSearchCard\")\n",
    "        \n",
    "        number_resume = len(find_resume)\n",
    "        resume_link = []\n",
    "        for i in range(number_resume):\n",
    "            link = find_resume[i].find('a')['href']\n",
    "            resume_link.append(link)\n",
    "        return resume_link\n",
    "        \n",
    "    def Open_link(self, input_links, input_array):\n",
    "        #input_array has resume information added\n",
    "        first_part_link = 'https://resumes.indeed.com'\n",
    "        number_resume = len(input_links)\n",
    "        for i in range(number_resume):\n",
    "            current_link = first_part_link + input_links[i]\n",
    "            self.driver.execute_script(\"window.open('');\")\n",
    "            #time.sleep(3)\n",
    "            self.driver.switch_to.window(driver.window_handles[1])\n",
    "            self.driver.get(current_link)\n",
    "            #time.sleep(3)\n",
    "            open_page = self.driver.page_source\n",
    "            \n",
    "            #start_time = time.time()\n",
    "            #*****************\n",
    "            #extract page information\n",
    "            self.Extract_page_info(i, open_page, input_array)\n",
    "            #end_time = time.time()\n",
    "            #print('time spent is: ', end_time - start_time)\n",
    "            #******************\n",
    "            \n",
    "            self.driver.close()\n",
    "            self.driver.switch_to.window(driver.window_handles[0])\n",
    "#Work Experience\n",
    "#Education\n",
    "#Skills\n",
    "#Additional Information\n",
    "    def _Create_list(self, page_content, category_type):\n",
    "        #input page_content, soup object\n",
    "        #category_type  string object\n",
    "        #return new_list  list type\n",
    "        category_list = ['Work Experience', 'Education',\n",
    "                            'Skills', 'Additional Information']\n",
    "        new_list =[]\n",
    "        if category_type == category_list[0]:\n",
    "            #Work Experience\n",
    "           \n",
    "            work_experience_list = page_content.find_all('div', class_ = 'rezemp-WorkExperience')\n",
    "            number_work = len(work_experience_list)\n",
    "            for i in range(number_work):\n",
    "                new_dict = {}\n",
    "                work_title = work_experience_list[i].find('div', \n",
    "                                                          class_ = 'rezemp-u-h4')\n",
    "                new_dict['title'] = work_title.get_text()\n",
    "                work_subtitle = work_experience_list[i].find('div', \n",
    "                                                             class_ = 'rezemp-WorkExperience-subtitle')\n",
    "                work_company = work_subtitle.find('div')\n",
    "                new_dict['company'] = work_company.get_text()\n",
    "                work_time = work_company.find_next_sibling('div')\n",
    "                if work_time ==None:\n",
    "                    new_dict['dates'] = 'NA'\n",
    "                else:\n",
    "                    new_dict['dates'] = work_time.get_text()\n",
    "                job_description = work_subtitle.find_next_sibling('div')\n",
    "                if job_description ==None:\n",
    "                    new_dict['description'] = 'NA'\n",
    "                else:\n",
    "                    new_dict['description'] = job_description.get_text().\\\n",
    "                                          replace(u'\\xa0', '').replace('\\t', '').replace('â€¢',' ')\n",
    "                new_list.append(new_dict)\n",
    "        elif category_type == category_list[1]:\n",
    "            #Education\n",
    "            #new_dict = {}\n",
    "            education_content = page_content.find('div', \n",
    "                                                  class_ = 'rezemp-ResumeDisplaySection-content')\n",
    "            degree_list = education_content.find_all('span', \n",
    "                                                     class_ = 'rezemp-ResumeDisplay-itemTitle')\n",
    "            school_list = education_content.find_all('div', \n",
    "                                                         class_ = 'rezemp-ResumeDisplay-university')\n",
    "            number_school = len(degree_list)\n",
    "            for i in range(number_school):\n",
    "                new_dict = {}\n",
    "                new_dict['title'] = degree_list[i].get_text()\n",
    "                school_name = school_list[i].find('span').get_text()\n",
    "                new_dict['school'] = school_name\n",
    "                school_location = school_list[i].find('span', \n",
    "                                                      class_ = 'rezemp-ResumeDisplay-universityLocation').get_text()\n",
    "                new_dict['location'] = school_location\n",
    "                school_dates = school_list[i].find('span', \n",
    "                                                   class_ = 'rezemp-ResumeDisplay-date').get_text()\n",
    "                new_dict['dates'] = school_dates\n",
    "                \n",
    "                new_list.append(new_dict)\n",
    "        elif category_type == category_list[2]:\n",
    "            #Skills\n",
    "            #new_dict = {}\n",
    "            skills = page_content.find('div', \n",
    "                                       class_ = 'rezemp-ResumeDisplaySection-content')\n",
    "            skills_string = skills.get_text()\n",
    "            new_list.append(skills_string)\n",
    "        else:\n",
    "            #Additional Information\n",
    "            new_dict = {}\n",
    "            additional_info = page_content.find('div', \n",
    "                                                class_ = 'rezemp-ResumeDisplaySection-content')\n",
    "            additional_info_string= additional_info.get_text()\n",
    "            new_list.append(additional_info_string)\n",
    "      \n",
    "        return new_list\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "    def Extract_page_info(self, person_index, current_page, input_array):\n",
    "        #current_page is source code of the webpage\n",
    "        #current_page = driver.page_source\n",
    "        #append the extracted information to input_array\n",
    "        soup = BeautifulSoup(current_page,'html.parser')\n",
    "        page_content = soup.find_all('div', class_ = 'rezemp-ResumeDisplaySection')\n",
    "        number_content = len(page_content)\n",
    "        \n",
    "        new_dict = {}\n",
    "        new_dict['index'] = person_index\n",
    "        summary_part = soup.find('div', class_ = 'rezemp-ResumeDisplay-body').find('div')\n",
    "        #***********************\n",
    "        title = summary_part.find('div', class_ = 'rezemp-u-h2')\n",
    "        new_dict['title'] = title.get_text()\n",
    "        title_sub = summary_part.find('div', class_ = 'rezemp-u-h4')\n",
    "        new_dict['subtitle'] = title_sub.get_text()\n",
    "        title_loc = title_sub.find_next_sibling('div')\n",
    "        new_dict['location'] = title_loc.get_text()\n",
    "        #**************************\n",
    "        #loop through page_content to find out which\n",
    "        #it belongs to the category for output\n",
    "        category_set = set(['Work Experience', 'Education',\n",
    "                            'Skills', 'Additional Information'])\n",
    "        #initialization\n",
    "        new_dict['work_experience'] = 'NA'\n",
    "        new_dict['education'] = 'NA'\n",
    "        new_dict['skills'] = 'NA'\n",
    "        new_dict['additional_information'] = 'NA'\n",
    "        \n",
    "        for i in range(number_content):\n",
    "            category_type = page_content[i].find('span', \n",
    "                                                 class_ = 'rezemp-ResumeDisplaySection-header').get_text()\n",
    "            if category_type in category_set:\n",
    "                if (category_type == 'Work Experience'):\n",
    "                    new_dict['work_experience'] = self._Create_list(page_content[i], category_type)\n",
    "                elif (category_type == 'Education'):\n",
    "                    new_dict['education'] = self._Create_list(page_content[i], category_type)\n",
    "                elif (category_type == 'Skills'):\n",
    "                    new_dict['skills'] = self._Create_list(page_content[i], category_type)\n",
    "                else:\n",
    "                    new_dict['additional_information'] = self._Create_list(page_content[i], category_type)\n",
    "                    \n",
    "        input_array.append(new_dict)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_scraper = Scraper_web()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. California\n",
    "#2. Washington\n",
    "#3. Oregon\n",
    "#4. Alabama\n",
    "#5. Arizona\n",
    "#6. Arkansas\n",
    "#7. Colorado\n",
    "#8. Connecticut\n",
    "#9. Delaware\n",
    "#10. Florida\n",
    "#11. Georgia\n",
    "#12. Hawaii\n",
    "#13. Idaho\n",
    "#14. Illinois\n",
    "#15. Indiana\n",
    "#16. Iowa\n",
    "#17. Kansas\n",
    "#18. Kentucky\n",
    "#19. Louisiana\n",
    "#20. Maine\n",
    "#21. Maryland\n",
    "#22. Massachusetts\n",
    "#23. Michigan\n",
    "#24. Minnesota\n",
    "#25. Mississippi\n",
    "#26. Missouri\n",
    "#27. Nebraska\n",
    "#28. Nevada\n",
    "#29. New Hampshire\n",
    "#30. New Jersey\n",
    "#31. New Mexico\n",
    "#32. New York\n",
    "#33. North Carolina\n",
    "#34. North Dakota\n",
    "#35. Ohio\n",
    "#36. Oklahoma\n",
    "#37. Pennsylvania\n",
    "#38. Rhode Island\n",
    "#39. South Carolina\n",
    "#40. South Dakota\n",
    "#41. Tennessee\n",
    "#42. Texas\n",
    "#43. Utah\n",
    "#44. Vermont\n",
    "#45. Virginia\n",
    "#46. Wisconsin\n",
    "#The other states do not have resume with this title\n",
    "title = ['Data Scientist', 'Data Analyst','Machine Learning Engineer', \n",
    "         'Data Engineer', 'Software Engineer', 'Engineer', 'Web Developer',\n",
    "         'Statistician', 'Electrical Engineer', 'Financial Analyst',\n",
    "         'Process Engineer', 'Computer Engineer']\n",
    "state_name = ['California', 'Washington', 'Oregon', 'Alabama',  'Arizona', \n",
    "              'Arkansas', 'Colorado','Connecticut', 'Delaware',\n",
    "              'Florida','Georgia', 'Hawaii', 'Idaho', 'Illinois',\n",
    "              'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n",
    "              'Maine', 'Maryland', 'Massachusetts','Michigan', 'Minnesota',\n",
    "              'Mississippi', 'Missouri', 'Nebraska', 'Nevada', 'New Hampshire',\n",
    "              'New Jersey', 'New Mexico', 'New York', 'North Carolina','North Dakota',\n",
    "              'Ohio', 'Oklahoma', 'Pennsylvania', 'Rhode Island', 'South Carolina',\n",
    "              'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont',\n",
    "              'Virginia', 'Wisconsin']\n",
    "            \n",
    "\n",
    "#about 5 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "what_content = title[0]\n",
    "where_content = state_name[2]\n",
    "test_scraper.Input_Search(driver, what_content, where_content)\n",
    "end_time = time.time()\n",
    "print('time spent is: ', end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "input_array =[]\n",
    "has_next = True\n",
    "start_time = time.time()\n",
    "page = test_scraper._Current_page()\n",
    "resume_link = test_scraper.Find_Links(page)\n",
    "test_scraper.Open_link(resume_link, input_array)\n",
    "i=1\n",
    "while has_next:\n",
    "    page = test_scraper._Next_page()\n",
    "    #time.sleep(5)\n",
    "    sys.stdout.write(\" P\"+str(i))\n",
    "    i +=1\n",
    "    has_next = page\n",
    "    if has_next:\n",
    "        resume_link = test_scraper.Find_Links(page)\n",
    "        test_scraper.Open_link(resume_link, input_array)\n",
    "end_time = time.time()\n",
    "print('Total time spent is: ', end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "print(len(input_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test_scraper.driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "save_directory ='./scraper_data/'\n",
    "file_index = 3\n",
    "name_base = 'file'\n",
    "file_name = save_directory + name_base + str(file_index)+'.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open(file_name, 'w') as f:\n",
    "    json.dump(input_array, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title in search is:  Engineer\n",
      "No next element\n",
      "j is:  43 state is: Vermont  Total time spent is:  253.47414088249207\n",
      "page has error information\n",
      "j is:  44 state is: Virginia  Total time spent is:  537.6544470787048\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-40803a8cd85f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_next\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mresume_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFind_Links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mtest_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'j is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'state is:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' Total time spent is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a3ddaaae934c>\u001b[0m in \u001b[0;36mOpen_link\u001b[0;34m(self, input_links, input_array)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m#*****************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m#extract page information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtract_page_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_page\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;31m#end_time = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m#print('time spent is: ', end_time - start_time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a3ddaaae934c>\u001b[0m in \u001b[0;36mExtract_page_info\u001b[0;34m(self, person_index, current_page, input_array)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mnew_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mnew_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperson_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0msummary_part\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rezemp-ResumeDisplay-body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;31m#***********************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_part\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rezemp-u-h2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "#j=32 has some issue\n",
    "#<50 for data scientist    0\n",
    "#[50,100) for data analyst  1\n",
    "#[150,200) machine learning engineer  2\n",
    "#[100, 150) Data Engineer 3\n",
    "#[200, 250)  Software Engineer 4\n",
    "#[250,300] Engineer   5\n",
    "save_directory ='./scraper_data/'\n",
    "file_index_base = 250\n",
    "title_number = 5\n",
    "what_content = title[title_number]\n",
    "name_base = 'file'\n",
    "number_states = len(state_name)\n",
    "j=43   \n",
    "print('Title in search is: ', what_content)\n",
    "while j<number_states:\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(new_link)\n",
    "    \n",
    "    test_scraper = Scraper_indeed()\n",
    "    #what_content = title[title_number]\n",
    "    where_content = state_name[j]\n",
    "    test_scraper.Input_Search(driver, what_content, where_content)\n",
    "    time.sleep(25)\n",
    "    input_array =[]\n",
    "    has_next = True\n",
    "    start_time = time.time()\n",
    "    page = test_scraper._Current_page()\n",
    "    resume_link = test_scraper.Find_Links(page)\n",
    "    test_scraper.Open_link(resume_link, input_array)\n",
    "    i=1\n",
    "    while has_next:\n",
    "        page = test_scraper._Next_page()\n",
    "        has_next = page\n",
    "        if has_next:\n",
    "            resume_link = test_scraper.Find_Links(page)\n",
    "            test_scraper.Open_link(resume_link, input_array)\n",
    "    end_time = time.time()\n",
    "    print('j is: ', j, 'state is:', where_content, ' Total time spent is: ', end_time - start_time)\n",
    "    test_scraper.driver.close()\n",
    "    \n",
    "    file_name = save_directory + name_base + str(j+file_index_base)+'.json'\n",
    "    with open(file_name, 'w') as f:\n",
    "        json.dump(input_array, f)\n",
    "    j +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=71.0.3578.98)\n  (Driver info: chromedriver=2.45.615355 (d5698f682d8b2742017df6c81e0bd8e6a3063189),platform=Mac OS X 10.13.3 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a5814e7cabc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \"\"\"\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLOSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    310\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mexception_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mUnexpectedAlertPresentException\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'alert'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=71.0.3578.98)\n  (Driver info: chromedriver=2.45.615355 (d5698f682d8b2742017df6c81e0bd8e6a3063189),platform=Mac OS X 10.13.3 x86_64)\n"
     ]
    }
   ],
   "source": [
    "test_scraper.driver.close()\n",
    "test_scraper.driver.switch_to.window(driver.window_handles[0])\n",
    "test_scraper.driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=71.0.3578.98)\n  (Driver info: chromedriver=2.45.615355 (d5698f682d8b2742017df6c81e0bd8e6a3063189),platform=Mac OS X 10.13.3 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-58f3b82bdde8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_scraper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \"\"\"\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLOSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    310\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mexception_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mUnexpectedAlertPresentException\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'alert'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=71.0.3578.98)\n  (Driver info: chromedriver=2.45.615355 (d5698f682d8b2742017df6c81e0bd8e6a3063189),platform=Mac OS X 10.13.3 x86_64)\n"
     ]
    }
   ],
   "source": [
    "test_scraper.driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
